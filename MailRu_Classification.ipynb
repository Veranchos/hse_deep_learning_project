{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle competitions download -c deepnlp-hse-course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \\*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('unsupervised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r''[^А-Яа-я.!? ]', ' ',  text)\n"
    "    words = [word for word in word_tokenize(text)]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['norm'] = df['question'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trash(text):\n",
    "    trash = re.findall(r'[^а-яa-zё|\\s|-]+', str(text))\n",
    "    if trash:\n",
    "        return trash\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trash'] = df['norm'].apply(find_trash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trash = df[df.trash.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fast_text = FastText([text.split(' ') for text in df['norm']], size=300,window=8, min_n=2,\n",
    "                                   max_n=8, sorted_vocab=1,negative=10, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text.wv.most_similar('путин')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'fasttext_new.model'\n",
    "fast_text.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'fasttext_new.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['trash'] = train['norm'].apply(find_trash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['trash'] = test['norm'].apply(find_trash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash = []\n",
    "for i in test['trash']:\n",
    "    if i:\n",
    "        trash += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash = list(set(trash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train['trash']:\n",
    "    if i:\n",
    "        trash += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash = list(set(trash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash_str = ''\n",
    "for i in trash:\n",
    "    trash_str += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['norm'] = train['question'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['norm'] = test['question'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokens'] = train['norm'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r''[^А-Яа-я.!? ]', ' ',  text)\n",
    "    words = wordpunct_tokenize(text.lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tokens'] = test['norm'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(train['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_words = []\n",
    "for sent in words:\n",
    "    for word in sent:\n",
    "        if not word in model.wv.vocab:\n",
    "            unk_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector  = np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words = []\n",
    "for word in tqdm(train_list):\n",
    "    vector = model.wv[word]\n",
    "    comparison = np.array_equal(vector, zero_vector)\n",
    "    if comparison:\n",
    "        unknown_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(test['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(train['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listmerge3(lstlst):\n",
    "    all_list=[]\n",
    "    for lst in lstlst:\n",
    "          all_list.extend(lst)\n",
    "    return all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test=listmerge3(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(set(list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test=listmerge3(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(set(train_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = train_list + test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list(set(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {'PAD' : 0}\n",
    "for word in data_list:\n",
    "    word2index[word] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(Dataset):\n",
    "    \n",
    "    def __init__(self, x_data, y_data, word2index, sequence_length=32, pad_token='PAD', verbose=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.x_data = []\n",
    "        self.y_data = y_data\n",
    "        self.ft = model.wv\n",
    "        \n",
    "        self.word2index = word2index\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.pad_index = self.word2index[self.pad_token] #here is 0\n",
    "        \n",
    "        self.load(x_data, verbose=verbose)\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text):\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r''[^А-Яа-я.!? ]', ' ',  text)\n",
    "        \n",
    "        words = wordpunct_tokenize(text.lower())\n",
    "        \n",
    "        return words\n",
    "        \n",
    "    def load(self, data, verbose=True):\n",
    "        \n",
    "        data_iterator = tqdm(data, desc='Loading data', disable=not verbose)\n",
    "        \n",
    "        for text in data_iterator:\n",
    "            \n",
    "            words = self.process_text(text)\n",
    "            \n",
    "            indexed_words = self.indexing(words)\n",
    "            \n",
    "            self.x_data.append(indexed_words)\n",
    "    \n",
    "    def indexing(self, tokenized_text):\n",
    "\n",
    "        # здесь мы не используем токен UNK, потому что мы мы его специально не учили\n",
    "        # становится непонятно какой же эмбеддинг присвоить неизвестному слову,\n",
    "        # поэтому просто выбрасываем наши неизветсные слова\n",
    "                \n",
    "        return [self.word2index[token] for token in tokenized_text if token in self.word2index]\n",
    "                \n",
    "    \n",
    "    def padding(self, sequence):\n",
    "        \n",
    "        # Ограничить длину self.sequence_length\n",
    "        sequence_length = self.sequence_length\n",
    "        \n",
    "        if len(sequence) > sequence_length:\n",
    "            sequence = sequence[:sequence_length] \n",
    "            \n",
    "        # если длина меньше максимальной - западить\n",
    "        if len(sequence) < sequence_length:\n",
    "            sequence += (sequence_length - len(sequence))*[self.pad_index]\n",
    "                         \n",
    "        return sequence\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.x_data[idx]\n",
    "        x = self.padding(x)\n",
    "        x = torch.Tensor(x).long()\n",
    "\n",
    "        y = self.y_data[idx]\n",
    "        \n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation = train_test_split(train.question, train.main_category, test_size=0.1)\n",
    "\n",
    "train_dataset = WordData(list(x_train), list(y_train), word2index)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "validation_dataset = WordData(list(x_validation), list(y_validation), word2index)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64)\n",
    "\n",
    "test_dataset = WordData(list(test.question), np.zeros((test.shape[0])), word2index)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in test_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = train.main_category.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2word = {}\n",
    "for key, value in word2index.items():\n",
    "    ind2word[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index['рубен']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300\n",
    "vectors = np.zeros((len(word2index), dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(len(word2index)):\n",
    "    vector = model.wv[ind2word[num]]\n",
    "    vectors[num] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAverageNetwork(torch.nn.Module):\n",
    "    \n",
    "    # def __init__(self, embedding_matrix, n_classes):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, n_classes, drop_prob=0.3):\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #fasttext эмбеддинги\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.Tensor(vectors))\n",
    "        \n",
    "\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.hidden2label = torch.nn.Linear(hidden_dim*2, 32)\n",
    "        self.cnn_1 = torch.nn.Conv1d(in_channels=600, out_channels=300, kernel_size=2)\n",
    "        \n",
    "        self.cnn_2 = torch.nn.Conv1d(in_channels=600, out_channels=300, kernel_size=3)\n",
    "        \n",
    "        self.cnn_3 = torch.nn.Conv1d(in_channels=600, out_channels=300, kernel_size=4)\n",
    "        \n",
    "        self.cnn_4 = torch.nn.Conv1d(in_channels=600, out_channels=300, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(300, 128)\n",
    "        self.fc1_bn = torch.nn.BatchNorm1d(128)\n",
    "        self.fc2 = torch.nn.Linear(128, 28)\n",
    " \n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        #прячем часть эмбеддингов\n",
    "#         embeds = self.dropout(embeds)\n",
    "        \n",
    "        #запаковываем \n",
    "        seq_lengths = torch.LongTensor(list(map(len, embeds)))\n",
    "        packed_embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, seq_lengths.cpu().numpy(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        #подаём в lstm\n",
    "        lstm_packed_output, _ = self.lstm(packed_embeds)\n",
    "        #распаковываем\n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_packed_output, batch_first=True)\n",
    "    \n",
    "#         #транспонируем, чтобы передать в cnn\n",
    "        transposed_lstm_out = lstm_out.transpose(1,2)\n",
    "\n",
    "        #добавляем 4 параллельные свёртки и пулинги к ним\n",
    "        cnn_1 = F.relu(self.cnn_1(transposed_lstm_out)) \n",
    "        maxpool_1 = torch.nn.MaxPool1d(kernel_size=cnn_1.shape[2])\n",
    "        maxpool1_out = maxpool_1(cnn_1)\n",
    "        \n",
    "        cnn_2 = F.relu(self.cnn_2(transposed_lstm_out))\n",
    "        maxpool_2 = torch.nn.MaxPool1d(kernel_size=cnn_2.shape[2])\n",
    "        maxpool2_out = maxpool_2(cnn_2)\n",
    "        \n",
    "        cnn_3 = F.relu(self.cnn_3(transposed_lstm_out))\n",
    "        maxpool_3 = torch.nn.MaxPool1d(kernel_size=cnn_3.shape[2])\n",
    "        maxpool3_out = maxpool_3(cnn_1)\n",
    "        \n",
    "        cnn_4 = F.relu(self.cnn_4(transposed_lstm_out))\n",
    "        maxpool_4 = torch.nn.MaxPool1d(kernel_size=cnn_4.shape[2])\n",
    "        maxpool4_out = maxpool_4(cnn_4)\n",
    "        \n",
    "        конкатенируем результаты\n",
    "        cnn_out =  torch.cat([maxpool1_out, maxpool2_out, maxpool3_out, maxpool4_out], 2)\n",
    "        \n",
    "        pred = cnn_out.transpose(1, 2)\n",
    "        \n",
    "        #два линейных слоя на выходе\n",
    "        pred = transposed_lstm_out[-1]\n",
    "        print(pred.shape)\n",
    "        pred = self.fc2(F.relu(self.fc1_bn(self.hidden2label(pred))))\n",
    "        pred = F.relu(self.fc1_bn(self.fc1(pred[:, -1, :])))\n",
    "        \n",
    "        pred = self.fc2(pred)\n",
    "        pred = F.relu(pred)\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2index) + 1\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "n_layers = 2\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepAverageNetwork(vocab_size, embedding_dim, hidden_dim, n_layers, n_classes)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    x = x.to(device)\n",
    "    pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "losses = []\n",
    "best_test_loss = 10.\n",
    "\n",
    "test_f1 = []\n",
    "\n",
    "for n_epoch in range(epochs):\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_targets = []\n",
    "    test_pred_class = []\n",
    "    \n",
    "    progress_bar = tqdm_notebook(total=len(train_loader.dataset), desc='Epoch {}'.format(n_epoch + 1))\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        progress_bar.set_postfix(train_loss = np.mean(losses[-500:]))\n",
    "\n",
    "        progress_bar.update(x.shape[0])\n",
    "        \n",
    "    progress_bar.close()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for x, y in validation_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred = model(x)\n",
    "\n",
    "            pred = pred.cpu()\n",
    "            y = y.cpu()\n",
    "\n",
    "            test_targets.append(y.numpy())\n",
    "            test_pred_class.append(np.argmax(pred, axis=1))\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "        \n",
    "    mean_test_loss = np.mean(test_losses)\n",
    "\n",
    "    test_targets = np.concatenate(test_targets).squeeze()\n",
    "    test_pred_class = np.concatenate(test_pred_class).squeeze()\n",
    "\n",
    "    f1 = f1_score(test_targets, test_pred_class, average='micro')\n",
    "\n",
    "    test_f1.append(f1)\n",
    "    \n",
    "    print()\n",
    "    print('Losses: train - {:.3f}, test - {:.3f}'.format(np.mean(train_losses), mean_test_loss))\n",
    "\n",
    "    print('F1 test - {:.3f}'.format(f1))\n",
    "        \n",
    "    # Early stopping:\n",
    "    if mean_test_loss < best_test_loss:\n",
    "        best_test_loss = mean_test_loss\n",
    "    else:\n",
    "        print('Early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for x, _ in test_loader:\n",
    "\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred = model(x)\n",
    "\n",
    "        pred = pred.cpu()\n",
    "        \n",
    "        predictions.append(np.argmax(pred, axis=1))\n",
    "        \n",
    "predictions = np.concatenate(predictions).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['main_category'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['index', 'main_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
